[**ä¸­æ–‡è¯´æ˜**](README.md) | [**English**](README_en.md)

# FG-CLIP 2: ä¸­è‹±åŒè¯­è§†è§‰è¯­è¨€å¯¹é½æ¨¡å‹

æœ¬ä»“åº“æ˜¯FG-CLIPåŠFG-CLIP 2çš„å®˜æ–¹å®ç°ç‰ˆæœ¬ï¼Œä½œä¸ºæ–°ä¸€ä»£æ–‡æœ¬-å›¾åƒè·¨æ¨¡æ€æ¨¡å‹ï¼Œåœ¨ç»†ç²’åº¦ç†è§£æ–¹é¢è¡¨ç°å“è¶Šã€‚FG-CLIP 2 æ”¯æŒä¸­è‹±åŒè¯­ï¼Œåœ¨ 29 ä¸ªæ•°æ®é›†å’Œ 8 ç±»å¤šæ ·åŒ–ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹è¶…è¶ŠåŒ…æ‹¬SigLIP 2 å’Œ MetaCLIP 2åœ¨å†…çš„å¼ºåŠ›åŸºçº¿æ¨¡å‹ï¼Œåœ¨ä¸¤ç§è¯­è¨€ä»»åŠ¡ä¸­å‡å–å¾—ç›®å‰çš„æœ€ä½³æ€§èƒ½ã€‚

**[FG-CLIP 2: A Bilingual Fine-grained Vision-language Alignment Model](https://arxiv.org/abs/2510.10921)** 
</br>
Chunyu Xie*, Bin Wang*, Fanjing Kong, Jincheng Li, Dawei Liang, Ji Ao, Dawei Lengâ€ , Yuhui Yin (*Equal Contribution, âœCorresponding Author)
</br>
[![arXiv](https://img.shields.io/badge/arXiv-2510.10921-b31b1b.svg)](https://arxiv.org/abs/2510.10921)
[![HF-model](https://img.shields.io/badge/Model-CollectionğŸ¤—-yellow.svg)](https://huggingface.co/collections/qihoo360/fg-clip-2-68ecbf9c548623bb78bc7913)
[![HF-data](https://img.shields.io/badge/Benchmark-CollectionğŸ¤—-yellow.svg)](https://huggingface.co/collections/qihoo360/fg-clip-2-68ecbf9c548623bb78bc7913)
[![API+MCP](https://img.shields.io/badge/API/MCP-FG--CLIPv2-green.svg)](https://research.360.cn/sass/index)

**[FG-CLIP: Fine-Grained Visual and Textual Alignment](https://arxiv.org/abs/2505.05071)** ([code branch: v1.0](https://github.com/360CVGroup/FG-CLIP/tree/v1.0))
</br>
Chunyu Xie*, Bin Wang*, Fanjing Kong, Jincheng Li, Dawei Liang, Gengshen Zhang, Dawei Lengâ€ , Yuhui Yin (*Equal Contribution, âœCorresponding Author)
</br>
[![arXiv](https://img.shields.io/badge/arXiv-2505.05071-b31b1b.svg)](https://arxiv.org/abs/2505.05071)
[![ICML](https://img.shields.io/badge/ICML-2025-blue.svg)](https://icml.cc/Conferences/2025)
[![HF-model](https://img.shields.io/badge/Model-CollectionğŸ¤—-yellow.svg)](https://huggingface.co/collections/qihoo360/fg-clip-681da45d4acfb65c240a6d08)
[![HF-data](https://img.shields.io/badge/Data-FineHARDğŸ¤—-yellow.svg)](https://huggingface.co/datasets/qihoo360/FineHARD)
[![DeepWiki](https://img.shields.io/badge/DeepWiki-FG--CLIP-blue.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McDcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==)](https://deepwiki.com/360CVGroup/FG-CLIP)

 <p align="center">
  <img src="./use_imgs/FGCLIP2_compare_all_n.png"  width="500" height="440"/>
</p>

## ğŸ”¥ æ–°é—»
- ğŸš€ **[2025/10/14]** æˆ‘ä»¬å·²ä¸Šä¼ FG-CLIP 2ä»£ç å’Œæ¨¡å‹æƒé‡
- ğŸš€ **[2025/10/14]** æˆ‘ä»¬å‘å¸ƒäº†è®ºæ–‡ [FG-CLIP 2: A Bilingual Fine-grained Vision-language Alignment Model](https://arxiv.org/abs/2510.10921)
- ğŸš€ **[2025/09/29]** æˆ‘ä»¬åˆšåˆšå¼€æºäº†FG-CLIPçš„MCPæœåŠ¡å™¨å®ç°, æ›´å¤šç»†èŠ‚è¯·ç‚¹å‡» [FGCLIP-MCP](https://github.com/360CVGroup/FGCLIP-MCP)
- ğŸš€ **[2025/07/29]** æˆ‘ä»¬æä¾›FG-CLIP 2 baseæ¨¡å‹çš„APIè®¿é—®ï¼Œè¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºFG-CLIP, æ›´å¤šç»†èŠ‚è¯·æŸ¥çœ‹ [research.360.cn](https://research.360.cn/sass/index)
- ğŸš€ **[2025/07/09]** æˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªæ¼”ç¤ºdemoï¼Œåˆ†åˆ«é’ˆå¯¹ [fine-grained retrieval](https://huggingface.co/spaces/qihoo360/FG-CLIP-Retrieval-demo) å’Œ [dense feature display](https://huggingface.co/spaces/qihoo360/FG-CLIP-Densefeature-demo)
- ğŸš€ **[2025/05/09]** æˆ‘ä»¬å·²å°†æ¨¡å‹ä¸Šä¼ åˆ° ğŸ¤—(https://huggingface.co/qihoo360/fg-clip-large)ï¼Œå¯ä»¥æ”¯æŒå¿«æ·ä½¿ç”¨ï¼
- ğŸš€ **[2025/05/09]** æˆ‘ä»¬å·²æ›´æ–°FG-CLIP githubä»“åº“ï¼Œç°åœ¨æ‚¨å¯ä»¥æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹äº†ï¼
- ğŸš€ **[2025/05/09]** æˆ‘ä»¬å‘å¸ƒäº†è®ºæ–‡ [FG-CLIP: Fine-Grained Visual and Textual Alignment](https://arxiv.org/abs/2505.05071).
- ğŸš€ **[2025/05/02]** FG-CLIPè¢«ICML'25ä¼šè®®æ¥æ”¶ã€‚


<!-- ## Overview




Fine-grained vision-language understanding requires precise alignment between visual content and linguistic descriptions, a capability that remains limited in current models, particularly in non-English settings. While models like CLIP perform well on global alignment, they often struggle to capture fine-grained details in object attributes, spatial relations, and linguistic expressions, with limited support for bilingual comprehension. To address these challenges, we introduce FG-CLIP 2, a bilingual vision-language model designed to advance fine-grained alignment for both English and Chinese. The key ingredients of FG-CLIP 2 are summarized below.

- Rich Fine-Grained Supervision. Including region-text matching and long-caption modeling, alongside multiple discriminative objectives. We further introduce the Textual Intra-modal Contrastive (TIC) loss to better distinguish semantically similar captions.
- Bilingual Multimodal Data. Trained on a carefully curated mixture of large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual performance.
- Performance. Extensive experiments on 29 datasets across 8 tasks show that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results in both languages.
- Chinese Multimodal Benchmark. To enable rigorous evaluation, we present a new benchmark for Chinese multimodal understanding, featuring long-caption retrieval and bounding box classification. -->



<!-- ## Model Performance -->
<!-- ### Long/short caption image-text retrieval, and zero-shot image classification..  -->



## Contents
- [æ¨¡å‹æ¶æ„](#æ¨¡å‹æ¶æ„)
- [å®‰è£…](#å®‰è£…)
- [æ¨¡å‹ä»“åº“](#æ¨¡å‹ä»“åº“)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è®­ç»ƒ](#è®­ç»ƒ)
- [è¯„æµ‹](#è¯„æµ‹)



## æ¨¡å‹æ¶æ„

æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸€ä¸ªä¸¤é˜¶æ®µåˆ†å±‚å­¦ä¹ æ¡†æ¶ï¼Œä»å…¨å±€è¯­ä¹‰åˆ°ç»†ç²’åº¦ç»†èŠ‚ï¼Œé€æ­¥å¢å¼ºè§†è§‰-è¯­è¨€å¯¹é½èƒ½åŠ›ã€‚

**ç¬¬ä¸€é˜¶æ®µï¼šå…¨å±€è¯­ä¹‰å¯¹é½**  
æˆ‘ä»¬ä»å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬å¯¹å¼€å§‹ï¼Œæ¯å¯¹æ•°æ®å‡åŒ…å«ä¸€ä¸ª**çŸ­æ–‡æœ¬æè¿°**ï¼ˆç”¨äºç®€æ´çš„åœºæ™¯çº§æè¿°ï¼‰å’Œä¸€ä¸ª**é•¿æ–‡æœ¬æè¿°**ï¼ˆç”¨äºä¸°å¯Œçš„ä¸Šä¸‹æ–‡ç»†èŠ‚ï¼‰ã€‚åœ¨æ­¤åŒè¯­è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯å®ç°å¼ºå¤§çš„å…¨å±€å¯¹é½ï¼Œä¸ºè‹±æ–‡å’Œä¸­æ–‡çš„è·¨æ¨¡æ€ç†è§£å¥ å®šåšå®åŸºç¡€ã€‚

**ç¬¬äºŒé˜¶æ®µï¼šç»†ç²’åº¦è§†è§‰-è¯­è¨€å­¦ä¹ **  
åœ¨å…¨å±€å¯¹é½è¡¨ç¤ºçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥åŒºåŸŸçº§ç›‘ç£ä¿¡å·å’Œå¤šç§ç»†ç²’åº¦ç›®æ ‡ï¼Œä»¥å¼ºåŒ–å±€éƒ¨å¯¹åº”å…³ç³»ã€‚å…·ä½“åŒ…æ‹¬ï¼š

- **ç»†ç²’åº¦è§†è§‰å­¦ä¹ **ï¼šé€šè¿‡ RoIAlign æå–çš„åŒºåŸŸç‰¹å¾ä¸çŸ­è¯­çº§æè¿°è¿›è¡ŒåŒºåŸŸ-æ–‡æœ¬å¯¹é½ã€‚
- **ç»†ç²’åº¦æ–‡æœ¬å­¦ä¹ **ï¼šåˆ©ç”¨å±æ€§æ‰°åŠ¨ç”Ÿæˆçš„ hard negative æ ·æœ¬ï¼ŒåŒºåˆ†ç»†å¾®çš„æ–‡æœ¬å·®å¼‚ã€‚
- **å¸¦å…¨å±€é˜ˆå€¼åŒæ­¥çš„è·¨æ¨¡æ€æ’åºæŸå¤±**ï¼šé‡‡ç”¨åŠ¨æ€è¾¹è·çš„æ’åºæŸå¤±ï¼Œå¹¶é€šè¿‡å…¨å±€åŒæ­¥çš„é˜ˆå€¼å®ç°ç¨³å®šçš„ hard negative æŒ–æ˜ã€‚
- **æ–‡æœ¬æ¨¡æ€å†…å¯¹æ¯”æŸå¤±**ï¼šåœ¨å•ä¸€è¯­è¨€å†…éƒ¨è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œä»¥åŒºåˆ†è¯­ä¹‰ç›¸è¿‘ä½†ä¸åŒçš„åŒºåŸŸæè¿°ã€‚

<p align="center">
  <img src="./use_imgs/framework.png" width="80%"/>
</p>

## å®‰è£…

```shell
conda create -n FGCLIP2 python=3.10 -y
conda activate FGCLIP2
cd FG-CLIP && pip install -e .
```

## æ¨¡å‹ä»“åº“


|æ¨¡å‹ |           è§†è§‰ç¼–ç å™¨           |                       æ¨¡å‹æƒé‡                     |                           æ¼”ç¤ºç•Œé¢                           |
|:-----------|:-----------------------:|:---------------------------------------------------------:|:--------------------------------------------------------:|
| FG-CLIP-Base   | vit-base-patch16-224 | [ğŸ¤—Huggingface](https://huggingface.co/qihoo360/fg-clip-base)  | [Retrieval](https://huggingface.co/spaces/qihoo360/FG-CLIP-Retrieval-demo) & [Dense Feature](https://huggingface.co/spaces/qihoo360/FG-CLIP-Densefeature-demo) |
|  FG-CLIP-Large   | vit-large-patch14-336 | ğŸ¤—[Huggingface](https://huggingface.co/qihoo360/fg-clip-large)  |  |
| FG-CLIP2-Base   | vit-base-patch16 | [ğŸ¤—Huggingface](https://huggingface.co/qihoo360/fg-clip2-base)  | [Retrieval](https://huggingface.co/spaces/qihoo360/FG-CLIP2-Retrieval-demo) & [Dense Feature](https://huggingface.co/spaces/qihoo360/FG-CLIP2-Densefeature-demo) |
|  FG-CLIP2-Large   | vit-large-patch16 | [ğŸ¤—Huggingface](https://huggingface.co/qihoo360/fg-clip2-large)  |  |
|  FG-CLIP2-So400m   | vit-so400m-patch16 | [ğŸ¤—Huggingface](https://huggingface.co/qihoo360/fg-clip2-so400m)  |  |

## è¯„æµ‹åŸºå‡†

|æ•°æ®é›† |          é“¾æ¥          | 
|:-----------|:-----------------------:|
| LIT-CN   | [ğŸ¤—https://huggingface.co/datasets/qihoo360/LIT-CN](https://huggingface.co/datasets/qihoo360/LIT-CN)  | 
|  DCI-CN   |  ğŸ¤—[https://huggingface.co/datasets/qihoo360/DCI-CN](https://huggingface.co/datasets/qihoo360/DCI-CN)  | 
| DOCCI-CN   |  [ğŸ¤—https://huggingface.co/datasets/qihoo360/DOCCI-CN](https://huggingface.co/datasets/qihoo360/DOCCI-CN)  |
|  BoxClass-CN   |  [ğŸ¤—https://huggingface.co/datasets/qihoo360/BoxClass-CN](https://huggingface.co/datasets/qihoo360/BoxClass-CN)  | 

## å¿«é€Ÿå¼€å§‹ ğŸ¤—

### åŠ è½½æ¨¡å‹
```Shell
import torch
from PIL import Image
from transformers import (
    AutoImageProcessor,
    AutoTokenizer,
    AutoModelForCausalLM,
)


model_root = "fgclip2-base-patch16"
model = AutoModelForCausalLM.from_pretrained(model_root,trust_remote_code=True).cuda()

device = model.device

tokenizer = AutoTokenizer.from_pretrained(model_root)
image_processor = AutoImageProcessor.from_pretrained(model_root)

```


### æ£€ç´¢

```Shell
def determine_max_value(image):
    w,h = image.size
    max_val = (w//16)*(h//16)
    if max_val > 784:
        return 1024
    elif max_val > 576:
        return 784
    elif max_val > 256:
        return 576
    elif max_val > 128:
        return 256
    else:
        return 128

img_root = "cat_dfclor.jpg"
image = Image.open(img_root).convert("RGB")

image_input = image_processor(images=image, max_num_patches=determine_max_value(image), return_tensors="pt").to(device)

# NOTE Short captions: max_length=64 walk_type="short"(default)
# NOTE Long captions: max_length=196 walk_type="long"

captions = [
"ä¸€ä¸ªç®€çº¦é£æ ¼çš„å§å®¤è§’è½ï¼Œé»‘è‰²é‡‘å±è¡£æ¶ä¸ŠæŒ‚ç€å¤šä»¶ç±³è‰²å’Œç™½è‰²çš„è¡£ç‰©ï¼Œä¸‹æ–¹æ¶å­æ”¾ç€ä¸¤åŒæµ…è‰²é‹å­ï¼Œæ—è¾¹æ˜¯ä¸€ç›†ç»¿æ¤ï¼Œå·¦ä¾§å¯è§ä¸€å¼ é“ºæœ‰ç™½è‰²åºŠå•å’Œç°è‰²æ•å¤´çš„åºŠã€‚",
"ä¸€ä¸ªç®€çº¦é£æ ¼çš„å§å®¤è§’è½ï¼Œé»‘è‰²é‡‘å±è¡£æ¶ä¸ŠæŒ‚ç€å¤šä»¶çº¢è‰²å’Œè“è‰²çš„è¡£ç‰©ï¼Œä¸‹æ–¹æ¶å­æ”¾ç€ä¸¤åŒé»‘è‰²é«˜è·Ÿé‹ï¼Œæ—è¾¹æ˜¯ä¸€ç›†ç»¿æ¤ï¼Œå·¦ä¾§å¯è§ä¸€å¼ é“ºæœ‰ç™½è‰²åºŠå•å’Œç°è‰²æ•å¤´çš„åºŠã€‚",
"ä¸€ä¸ªç®€çº¦é£æ ¼çš„å§å®¤è§’è½ï¼Œé»‘è‰²é‡‘å±è¡£æ¶ä¸ŠæŒ‚ç€å¤šä»¶ç±³è‰²å’Œç™½è‰²çš„è¡£ç‰©ï¼Œä¸‹æ–¹æ¶å­æ”¾ç€ä¸¤åŒè¿åŠ¨é‹ï¼Œæ—è¾¹æ˜¯ä¸€ç›†ä»™äººæŒï¼Œå·¦ä¾§å¯è§ä¸€å¼ é“ºæœ‰ç™½è‰²åºŠå•å’Œç°è‰²æ•å¤´çš„åºŠã€‚",
"ä¸€ä¸ªç¹å¿™çš„è¡—å¤´å¸‚åœºï¼Œæ‘Šä½ä¸Šæ‘†æ»¡æ°´æœï¼ŒèƒŒæ™¯æ˜¯é«˜æ¥¼å¤§å¦ï¼Œäººä»¬åœ¨å–§é—¹ä¸­è´­ç‰©ã€‚"
]
captions = [caption.lower() for caption in captions]

caption_input = tokenizer(captions, padding="max_length", max_length=196, truncation=True, return_tensors="pt").to(device)


with torch.no_grad():
  image_feature = model.get_image_features(**image_input)
  text_feature = model.get_text_features(**caption_input,walk_type="long")
  image_feature = image_feature / image_feature.norm(p=2, dim=-1, keepdim=True)
  text_feature = text_feature / text_feature.norm(p=2, dim=-1, keepdim=True)

logits_per_image = image_feature @ text_feature.T
logit_scale, logit_bias = model.logit_scale.to(text_feature.device), model.logit_bias.to(text_feature.device)
logits_per_image = logits_per_image * logit_scale.exp() + logit_bias
```
 <p align="left">
  <img src="use_imgs\cn_re_demo.png" width=100%/>
</p>

### å¯†é›†ç‰¹å¾æ•ˆæœå±•ç¤º

```Shell

import math
import matplotlib
matplotlib.use('Agg') 
import matplotlib.pyplot as plt

def resize_short_edge(image, target_size=2048):
    if isinstance(image, str):
        image = Image.open(image)
    width, height = image.size
    short_edge = min(width, height)

    if short_edge >= target_size:
        return image
    scale = target_size / short_edge
    new_width = int(width * scale)
    new_height = int(height * scale)
    resized_image = image.resize((new_width, new_height))
    return resized_image


img_root = "cat_dfclor.jpg"
image = Image.open(img_root).convert("RGB")
image = resize_short_edge(image,target_size=2048)

image_input = image_processor(images=image, max_num_patches=16384, return_tensors="pt").to(device)
captions = ["ç”µè„‘","é»‘çŒ«","çª—æˆ·","window","white cat","book"]

with torch.no_grad():
    dense_image_feature = model.get_image_dense_feature(**image_input)
    
    spatial_values = image_input["spatial_shapes"][0]
    real_h = spatial_values[0].item()
    real_w = spatial_values[1].item()
    real_pixel_tokens_num = real_w*real_h
    dense_image_feature = dense_image_feature[0][:real_pixel_tokens_num]
    captions = [caption.lower() for caption in captions]
    caption_input = tokenizer(captions, padding="max_length", max_length=64, truncation=True, return_tensors="pt").to(device)

    text_feature = model.get_text_features(**caption_input, walk_type="box")
    text_feature = text_feature / text_feature.norm(p=2, dim=-1, keepdim=True)
    dense_image_feature = dense_image_feature / dense_image_feature.norm(p=2, dim=-1, keepdim=True)

similarity = dense_image_feature @ text_feature.T
similarity = similarity.cpu()


num_classes = len(captions)
cols = 3
rows = (num_classes + cols - 1) // cols


aspect_ratio = real_w / real_h 

fig_width_inch = 3 * cols        
fig_height_inch = fig_width_inch / aspect_ratio * rows / cols  

fig, axes = plt.subplots(rows, cols, figsize=(fig_width_inch, fig_height_inch))
fig.subplots_adjust(wspace=0.01, hspace=0.01)

if num_classes == 1:
    axes = [axes]
else:
    axes = axes.flatten()

for cls_index in range(num_classes):
    similarity_map = similarity[:, cls_index].cpu().numpy()
    show_image = similarity_map.reshape((real_h, real_w))

    ax = axes[cls_index]
    ax.imshow(show_image, cmap='viridis', aspect='equal')  
    ax.set_xticks([])
    ax.set_yticks([])
    ax.axis('off')


for idx in range(num_classes, len(axes)):
    axes[idx].axis('off')

savename = "FGCLIP2_dfcolor_cat_all_2K.png"
plt.savefig(savename, dpi=150, bbox_inches='tight', pad_inches=0.05)
plt.close()
```

 <p align="left">
  <img src="use_imgs\FGCLIP2_dfcolor_cat_all_2K.png" width=100%/>
</p>

## è®­ç»ƒ

### æ•°æ®å‡†å¤‡

æˆ‘ä»¬æä¾›ä½¿ç”¨ [ğŸ¤—FineHARD dataset](https://huggingface.co/datasets/qihoo360/FineHARD) è¿›è¡Œç¬¬äºŒé˜¶æ®µè®­ç»ƒçš„ä»£ç ã€‚FineHARD æ•°æ®é›†åŒ…å«1200ä¸‡å¼ å›¾åƒã€4000ä¸‡ä¸ªå¸¦æœ‰ç»†ç²’åº¦åŒºåŸŸæè¿°çš„è¾¹ç•Œæ¡†ï¼Œä»¥åŠ1000ä¸‡ä¸ªhard negativeæ ·æœ¬ã€‚

å…³äºæ•°æ®å‡†å¤‡ï¼Œè¯·å‚è€ƒ [Data: FineHARD](data/data.md)


### å‡†å¤‡è®­ç»ƒ
æˆ‘ä»¬çš„è®­ç»ƒå’Œæ¨ç†ä»£ç å®Œå…¨åŸºäº Hugging Face æä¾›çš„ transformers ä»“åº“ï¼Œéå¸¸æ˜“äºä½¿ç”¨å’Œå¤ç°ã€‚æˆ‘ä»¬åœ¨ scripts ç›®å½•ä¸­æä¾›äº†è®­ç»ƒè„šæœ¬ã€‚
</br>
[ğŸ¤— Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.](https://github.com/huggingface/transformers)
</br>
æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬æ”¯æŒ zero2ã€tf32 åŠ é€Ÿå’Œ bf16 ç²¾åº¦ï¼ˆæ³¨æ„ fp16 ç²¾åº¦å¯èƒ½å¯¼è‡´æ¢¯åº¦ NANï¼‰ã€‚å¦‚æœæ‚¨ä¸æ»¡è¶³ä¸Šè¿°æ¡ä»¶ï¼Œè¯·å…³é—­ tf32 å¹¶ä½¿ç”¨ torchrun æ›¿ä»£ deepspeed å¯åŠ¨ã€‚
</br>
```Shell
bash scripts/train/stage2_fgclip2.sh
```


## è¯„æµ‹
### æ•°æ®å‡†å¤‡
ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½ share-captioner_coco_lcs_sam_1246k_1107.json 
https://huggingface.co/datasets/Lin-Chen/ShareGPT4V/blob/main/share-captioner_coco_lcs_sam_1246k_1107.json

ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½ CocoCaptions å¹¶æ”¾å…¥ data/coco/annotations/

https://github.com/tylin/coco-caption

ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½ COCO å¹¶æ”¾å…¥ data/coco

https://cocodataset.org/dataset

DCI çš„æè¿°æ¥è‡ªä»¥ä¸‹é“¾æ¥å¹¶æ”¾å…¥ data/densely_captioned_images

https://github.com/facebookresearch/DCI

ImageNet-1K æ¥è‡ªä»¥ä¸‹é“¾æ¥å¹¶æ”¾å…¥ data/IN1K_val

https://image-net.org/

ImageNet-v2 æ¥è‡ªä»¥ä¸‹é“¾æ¥å¹¶æ”¾å…¥ data/imagenetv2-matched-frequency-format-val

https://opendatalab.com/OpenDataLab/ImageNetV2/tree/main


```bash
bash scripts/eval/eval.sh
```




<!-- ## Acknowledgement -->
## æ‹›è˜ä¸­
æˆ‘ä»¬æ­£åœ¨æ‹›å‹Ÿå¤šæ¨¡æ€æ–¹å‘çš„å­¦æœ¯å®ä¹ ç”Ÿã€‚å¦‚æœ‰å…´è¶£ï¼Œè¯·å°†ç®€å†å‘é€è‡³ xiechunyu@360.cn.
## å¼•ç”¨
å¦‚æœæ‚¨åœ¨ç ”ç©¶æˆ–åº”ç”¨ä¸­å‘ç° FG-CLIP 2 å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ BibTeX å¼•ç”¨ï¼š

```
@article{xie2025fg2,
  title={FG-CLIP 2: A Bilingual Fine-grained Vision-language Alignment Model},
  author={Xie, Chunyu and Wang, Bin and Kong, Fanjing and Li, Jincheng and Liang, Dawei and Ao, Ji and Leng, Dawei and Yin, Yuhui},
  journal={arXiv preprint arXiv:2510.10921},
  year={2025}
}
```
```
@article{xie2025fg,
  title={FG-CLIP: Fine-Grained Visual and Textual Alignment},
  author={Xie, Chunyu and Wang, Bin and Kong, Fanjing and Li, Jincheng and Liang, Dawei and Zhang, Gengshen and Leng, Dawei and Yin, Yuhui},
  journal={arXiv preprint arXiv:2505.05071},
  year={2025}
}
```
